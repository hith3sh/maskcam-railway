# NOTE: Some values might be overriden via ENV vars (check maskcam/config.py)

[track-processor]
# Detections with score below this threshold will be discarded
detection-threshold=0.1
# Only vote defective/non-defective when detection score is above this
voting-threshold=0.75
# Smaller detections (in pixels) will be discarded
min-track-size=8
disable-tracker=0

[maskcam]
# Alert conditions
# Minimum tracks to even calculate defective-fraction
alert-min-visible-tracks=1
# More than this fraction of defective tracks will raise an alarm
alert-defective-fraction=0.25
# More than this tracks detected will raise alarm despite defective-fraction
alert-max-total-tracks=10

# Time to send statistics in seconds. Set smaller than fileserver-video-period
statistics-period=5
statistics-to-json-period=60
statistics-directory=/home/lab5/Desktop/inference_statistics/

# Time (in seconds) to restart statistics (and the whole Deepstream inference process)
# Set to 0 to disable / 24hs = 86400 seconds
timeout-inference-restart=86400
inference-log-interval=300

# Other valid inputs:
#  - CSI cameras like RaspiCam:
#    -> argus://0
#  - Any file:
#    -> file:///absolute/path/to/file.mp4
default-input=v4l2:///dev/video0

# Output/streaming video resolution. 1024x576 keeps 4k aspect ratio of 1.777
output-video-width=1024
output-video-height=576

# Run utils/gst_capabilities.sh and find video/x-raw entries
camera-framerate=30

# Only used for argus:// inputs
camera-flip-method=0

# Auto-calculate nvinfer's `interval` based on `camera-framerate` and `inference-max-fps`
# to avoid delaying the pipeline. This will override the fixed `interval` parameter below
# E.g: if framerate=30 and max-fps=14,
#      -> will set interval=2 so that inference runs only 1/3 of incoming frames
inference-interval-auto=1
# Set this value to the actual FPS bottleneck of the model. Only used if inference-interval-auto.
# e.g: run the model on a video file (instead of live camera source) to determine model's FPS on your device
inference-max-fps=14

udp-port-streaming=5400
# 2 ports for overlapping file-save processes
udp-ports-filesave=5401,5402

streaming-start-default=1
streaming-port=8554
streaming-path=/maskcam
streaming-clock-rate=90000
# Supported: MP4, H264, H265
# Recommended H264 for stability on video save
codec=H264

# Sequentially saving videos
fileserver-enabled=0
fileserver-port=8080
fileserver-video-period=30
fileserver-video-duration=35
fileserver-force-save=0
fileserver-ram-dir=/dev/shm
# Use /tmp/* to clean saved videos on system reboot
fileserver-hdd-dir=/home/lab5/Desktop/statistics

# IP or domain address that this device will show in info messages (logs and web frontend, for streaming and file downloading)
# Recommended: use env variable MASKCAM_DEVICE_ADDRESS to set this
device-address=0

[property]
gpu-id=0
net-scale-factor=0.0039215697906911373
model-color-format=0

# YOLOV11
onnx-file=yolov11-nano-nighttime/yolov11-weights/yolov11n.onnx
model-engine-file=yolov11-nano-nighttime/yolov11-weights/model_b1_gpu0_fp16.engine
labelfile-path=yolov11-nano-nighttime/yolov11-weights/labels.txt
custom-lib-path=yolov11-nano-nighttime/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so
parse-bbox-func-name=NvDsInferParseYolo
engine-create-func-name=NvDsInferYoloCudaEngineGet

# frames processed by nvinfer at once as a batch 
batch-size=1

## 0=FP32, 1=INT8, 2=FP16 mode
network-mode=2

# no of frames to skip between two inference operations
interval=1

gie-unique-id=1
process-mode=1
network-type=0
cluster-mode=2
maintain-aspect-ratio=1
symmetric-padding=1
num-detected-classes=2

# is-classifier=0
## 0=Group Rectangles, 1=DBSCAN, 2=NMS, 3= DBSCAN+NMS Hybrid, 4 = None(No clustering)
# Default: 2

# Skip inference these frames
#maintain-aspect-ratio=0
#scaling-filter=1
#scaling-compute-hw=1
#output-blob-names=2012

# Async mode doesn't make sense with our custom python tracker
classifier-async-mode=0


[class-attrs-all]
nms-iou-threshold=0.2

# Default: 0.4
pre-cluster-threshold=0.4
topk=4

